FRAGE 1:

Paper sagt:

The output vector of the second LSTM layer is transformed to the output target, i.e. the
rectified magnitude ratio mask, through a dense layer with sigmoid activation.

W채re die Netzarchitektur demnach:

lstm1
lstm2
linear
rrelu
sigmoid

oder

lstm1
lstm2
linear
sigmoid

maske 2 kan채le
real imagin채r teil
f체r jeden zeit freq punkt
werte von -inf +inf

statt sigmoid und rrelu -> tanh


FRAGE 2:

Paper sagt:

The training process was early-stopped with two epochs patience.

wie implementiere ich sowas

-> callback early stopping
kriterium: loss
checkpoints
checkpoint zum besten loss speichern






